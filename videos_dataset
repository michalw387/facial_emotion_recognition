import os
import random
import numpy as np
import cv2
import torch
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader

import config
from face_detection_from_photos import get_faces_from_images


class VideosDataset(Dataset):
    def __init__(self, frames, emotions):
        self.frames = torch.tensor(frames)
        self.emotions = torch.tensor(emotions)
        self.n_frames = len(self.frames)

    def __len__(self):
        return self.n_frames

    def __getitem__(self, index):
        return self.frames[index], self.emotions[index]


class LoadVideosDataset:
    def __init__(self, batch_size=10, shuffle=True, num_workers=2):
        self.dataset = self.get_videos()
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.num_workers = num_workers

    def print_dataset_item(self, index=0):
        print("Emotions:", self.dataset[index]["emotions"])
        print("Frames:", self.dataset[index]["faces"])

    def print_dataset_info(self):
        print("---Dataset info---")
        print("Dataset size:", len(self.dataset))
        print("Frames per person:", len(self.dataset[0]["faces"]))
        print("Frame size:", np.array(self.dataset[0]["faces"][0]).shape)
        different_emotions = set(
            np.array(
                [self.dataset[i]["emotions"] for i in range(len(self.dataset))]
            ).flatten()
        )
        print("Emotions:", [config.EMOTIONS_DICT[i] for i in different_emotions])
        print("-----------------")

    def normalize_dataset(self):
        for person in self.dataset:
            for i in range(len(person["faces"])):
                person["faces"][i] = person["faces"][i] / 255

    def get_test_data_loader(self):
        test_frames = self.get_test_set()

        frames = test_frames["faces"]
        emotions = test_frames["emotions"]

        for frame in frames:
            print(frame.shape)

        return DataLoader(
            VideosDataset(frames, emotions),
            batch_size=self.batch_size,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
        )

    def get_test_set(self) -> dict:
        return self.dataset.pop(random.randint(0, len(self.dataset) - 1))

    # def split_dataset(dataset):
    #     test_videos = dataset.pop(random.randint(0, len(dataset) - 1))

    #     train_videos = []

    #     for person in dataset:
    #         for frame, emotion in zip(person["faces"], person["emotions"]):
    #             train_videos.append({"face": frame, "emotion": emotion})

    #     random.shuffle(train_videos)

    #     return train_videos, test_videos

    def get_train_val_data_loaders(self):

        flatten_dataset = self.get_flatten_persons_from_dataset()

        train_frames, val_frames, train_emotions, val_emotions = train_test_split(
            flatten_dataset["frames"], flatten_dataset["emotions"], test_size=0.2
        )

        return (
            DataLoader(
                VideosDataset(train_frames, train_emotions),
                batch_size=self.batch_size,
                shuffle=self.shuffle,
                num_workers=self.num_workers,
            ),
            DataLoader(
                VideosDataset(val_frames, val_emotions),
                batch_size=self.batch_size,
                shuffle=self.shuffle,
                num_workers=self.num_workers,
            ),
        )

    def get_frames_and_emotions(self):
        frames, emotions = [], []

        for person in self.dataset:
            for frame, emotion in zip(person["faces"], person["emotions"]):
                frames.append(frame)
                emotions.append(emotion)

        return frames, emotions

    def get_flatten_persons_from_dataset(self):
        frames, emotions = [], []

        for person in self.dataset:
            for frame, emotion in zip(person["faces"], person["emotions"]):
                frames.append(frame)
                emotions.append(emotion)

        return {"frames": frames, "emotions": emotions}

    def get_videos(self, show_frames=False):
        print("---Loading videos files---")

        dataset = []
        for folder in os.listdir(config.VIDEOS_DIRECTORY):
            if config.MAX_FOLDERS is not None and len(dataset) >= config.MAX_FOLDERS:
                break
            # if len(dataset) >= config.MAX_FOLDERS:
            #     break

            print("Loading folder:", folder)

            selected_frames = self.get_one_person_videos(folder)

            frames = [frame["image"] for frame in selected_frames]
            emotions = [frame["emotion"] for frame in selected_frames]

            detected_faces = get_faces_from_images(
                frames,
                show_frames,
                labels_indexes=[frame["emotion"] for frame in selected_frames],
                crop=True,
                only_one_face=True,
            )

            dataset.append({"faces": detected_faces, "emotions": emotions})

        print("---Videos loaded---")

        return dataset

    def get_one_person_videos(self, folder):
        selected_frames = []

        for video_filename in os.listdir(config.VIDEOS_DIRECTORY + folder):
            if (
                config.MAX_VIDEOS_PER_PERSON is not None
                and len(selected_frames) >= config.MAX_VIDEOS_PER_PERSON
            ):
                break

            emotion_index = int(video_filename[6:8])

            video_path = os.path.join(config.VIDEOS_DIRECTORY + folder, video_filename)

            cap = cv2.VideoCapture(video_path)

            all_frames = []

            while cap.isOpened():
                ret, frame = cap.read()  # frame size: (720, 1280, 3)
                if ret:
                    all_frames.append(frame)
                else:
                    break

            used_frames = []

            fraction_video_size = (len(all_frames) - 1) // config.FRAMES_PER_VIDEO

            for offset in range(config.FRAMES_PER_VIDEO):
                i = random.randint(
                    fraction_video_size * offset + 10,
                    fraction_video_size * (offset + 1) - 10,
                )

                while i in used_frames:
                    i = random.randint(0, len(all_frames) - 1)

                selected_frames.append(
                    {"image": all_frames[i], "emotion": emotion_index}
                )

                used_frames.append(i)

        return selected_frames

    def show_frame_with_emotion(self, frame, emotion):
        cv2.imshow("frame", frame)
        print("Emotion:", emotion)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

    def save_dataset_to_file(self, filename="videos_dataset.npy"):
        np.save(filename, self.dataset)

    def load_dataset_from_file(self, filename="videos_dataset.npy"):
        dataset = np.load(filename, allow_pickle=True)
        self.dataset = dataset.tolist()
        return list(dataset)


if __name__ == "__main__":
    dataset = LoadVideosDataset()
    dataset.print_dataset_info()

    dataset.normalize_dataset()
    dataset.save_dataset_to_file("all_videos_dataset.npy")

    dataset.load_dataset_from_file("all_videos_dataset.npy")

    test_loader = dataset.get_test_data_loader()

    train_loader, val_loader = dataset.get_train_val_data_loaders()

    print("Test loader size:", len(test_loader))
    for frames, emotions in test_loader:
        print(frames.shape, emotions.shape)
        break

    print("Train loader size:", len(train_loader))
    for frames, emotions in train_loader:
        print(frames.shape, emotions.shape)
        break

    print("Val loader size:", len(val_loader))
    for frames, emotions in val_loader:
        print(frames.shape, emotions.shape)
        break
